# -*- coding: utf-8 -*-
"""
Batch í•™ìŠµ + ì˜ˆì¸¡ + ëª¨ë¸/ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ (Refactored)
- ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€ (Prev_Close)
- ì˜¨ë¼ì¸ ëŸ¬ë‹ ì•ˆì „ì„± ê°•í™” (alpha ê°±ì‹  ë°©ì‹ ìˆ˜ì •)
- ë°©í–¥ì„±(MCC) + ë³€í™”ìœ¨(SMAPE) ë³µí•© ë³´ìƒ(0.7â€†:â€†0.3)
- pred ë°°ì¹˜ëŠ” ì˜¤ì§ í‰ê°€ì—ë§Œ ì‚¬ìš© (í•™ìŠµ X)
- bulk DB ì €ì¥
"""

from __future__ import annotations

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import os, sys, logging
from dataclasses import dataclass
from typing import Iterable, List, Tuple
from pathlib import Path

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import numpy as np
import pandas as pd
from pandas.tseries.offsets import CustomBusinessDay
from sklearn.linear_model import SGDRegressor
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import matthews_corrcoef
from tabulate import tabulate

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  Django ORM bootstrap  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PROJECT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
sys.path.insert(0, PROJECT_DIR)
import django
os.environ.setdefault("DJANGO_SETTINGS_MODULE", "backend.settings")
django.setup()
from ai.models import AI_REWARD2

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  ë‚´ë¶€ ëª¨ë“ˆ  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from nomalization import (
    walk_forward_batches,
    mini_batch_normalization,
    load_model, save_model,
    load_scaler, save_scaler,
    load_or_init_scaler,
    add_sma_features
)

pd.set_option("display.max_rows", None)
pd.set_option("display.max_columns", None)
pd.set_option("display.width", None)
pd.set_option("display.max_colwidth", None)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  ìƒìˆ˜Â·ì „ì—­  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
LOGGER = logging.getLogger(__name__)
LOGGER.setLevel(logging.INFO)

np.random.seed(42)  # ì¬í˜„ì„± í™•ë³´ (ìš´ì˜ ì‹œ ì‚­ì œ)

EPS                     = 1e-8
ALPHA_MIN               = 1e-8
ALPHA_RESET             = 1e-4
DECAY_K                 = 0.5

# ë³´ìƒ ê°€ì¤‘ì¹˜: 0.7(MCC) Â· 0.3(SMAPE)
W_MCC, W_SMAPE          = 0.7, 0.3

EXTRA_SMA   = [f"SMA_{p}" for p in (5, 20, 60)]
FIT_COLS    = ["Open", "High", "Low", "Volume", "Prev_Close"] + EXTRA_SMA
TARGET_COL  = "Close"
KR_US_BDAY  = CustomBusinessDay(calendar="KRX")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  ê²°ê³¼ êµ¬ì¡°ì²´  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@dataclass
class BatchResult:
    train_period: Tuple[str, str]
    pred_period: Tuple[str, str]
    mcc: float
    smape: float
    reward: float
    y_true: np.ndarray
    y_pred: np.ndarray

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  ë³´ìƒÂ·Î± í—¬í¼  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _composite_reward(mcc: float, smape: float) -> float:
    """MCC(âˆ’1~1)Â·SMAPE(0~1) â†’ 0~1 ìŠ¤ì¼€ì¼ ë³´ìƒ"""
    mcc_norm   = max(mcc, -1.0)  # ì•ˆì •ì„±
    smape_norm = max(min(smape, 1.0), 0.0)
    # SMAPEëŠ” ì‘ì„ìˆ˜ë¡ ì¢‹ê¸° ë•Œë¬¸ì— (1 âˆ’ smape) ë¡œ ë’¤ì§‘ìŒ
    return W_MCC * ((mcc_norm + 1) / 2) + W_SMAPE * (1 - smape_norm)


def _update_alpha(cur_alpha: float, reward: float) -> float:
    """exp ê°ì‡ ë¡œ ê·œì œ ê°•ë„ ì—…ë°ì´íŠ¸, ALPHA_MIN ì´í•˜ë¡œëŠ” ì˜ë¼ëƒ„"""
    new_alpha = cur_alpha * np.exp(-DECAY_K * reward)
    return max(new_alpha, ALPHA_MIN)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  í•µì‹¬ í•¨ìˆ˜  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def partial_fit_batches(
    batches: Iterable[Tuple[pd.DataFrame, pd.DataFrame]] ,
    model:  SGDRegressor | None = None,
    scaler: MinMaxScaler  | None = None,
    verbose: bool = True,
) -> Tuple[SGDRegressor, MinMaxScaler, list[BatchResult]]:

    # â”€â”€ 1. ëª¨ë¸Â·ìŠ¤ì¼€ì¼ëŸ¬ ì´ˆê¸°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if model is None:
        model = SGDRegressor(
            loss="huber",
            epsilon=1.35,
            penalty="elasticnet",
            alpha=1e-6,          # ê·œì œ ê°•ë„
            l1_ratio=0.1,        # L1 10 %  /  L2 90 %
            learning_rate="invscaling",
            eta0=0.01,
            power_t=0.25,
            random_state=42,
        )

    # fit 1 Ã— scaler â†’ transform train / pred ì— ë™ì¼ ì ìš©
    if scaler is None:
        scaler = MinMaxScaler()

    results: list[BatchResult] = []

    # â”€â”€ 2. ë°°ì¹˜ ë£¨í”„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    for train_b, pred_b in batches:
        if train_b.empty or pred_b.empty:
            continue

        # (1) í”¼ì²˜ í™•ì¥(SMA)  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if "SMA_5" not in train_b.columns:
            train_b = add_sma_features(train_b, periods=(5, 20, 60))
            pred_b  = add_sma_features(pred_b,  periods=(5, 20, 60))

        # (2) ëˆ„ìˆ˜ ì°¨ë‹¨ìš© Prev_Close  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        train_b = train_b.copy()
        train_b["Prev_Close"] = train_b[TARGET_COL].shift(1)
        train_b.dropna(subset=["Prev_Close"], inplace=True)

        pred_b  = pred_b.copy()
        pred_b["Prev_Close"] = pred_b[TARGET_COL].shift(1)
        pred_b.dropna(subset=["Prev_Close"], inplace=True)

        # (3) ìŠ¤ì¼€ì¼ëŸ¬ ì ì§„ í•™ìŠµ + ë³€í™˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        scaler.partial_fit(train_b[FIT_COLS])

        X_train = pd.DataFrame(
            scaler.transform(train_b[FIT_COLS]),
            columns=FIT_COLS, index=train_b.index
        )
        y_train = train_b[TARGET_COL].values

        # ì¤‘ìš” í”¼ì²˜ ê°€ì¤‘ì¹˜(ì„ íƒ)  ----------------------
        X_train.loc[:, "Volume"] *= 3
        X_train.loc[:, ["SMA_5", "SMA_20", "SMA_60"]] *= 2

        # (4) ì˜¨ë¼ì¸ í•™ìŠµ  -----------------------------
        model.partial_fit(X_train, y_train)

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # â–  í‰ê°€ ë‹¨ê³„ : pred_b ê·¸ëŒ€ë¡œ ì‚¬ìš©  â– 
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if pred_b.empty:
            continue

        X_pred = pd.DataFrame(
            scaler.transform(pred_b[FIT_COLS]),
            columns=FIT_COLS, index=pred_b.index
        )
        X_pred.loc[:, "Volume"] *= 3
        X_pred.loc[:, ["SMA_5", "SMA_20", "SMA_60"]] *= 2

        y_true = pred_b[TARGET_COL].astype(float).values
        y_pred = model.predict(X_pred)

        # ìœ íš¨ì„± í•„í„°
        mask = ~np.isnan(y_true) & ~np.isnan(y_pred)
        if mask.sum() < 2:
            LOGGER.warning("âŒ Too few valid samples â†’ Batch skipped")
            continue
        y_true = y_true[mask]
        y_pred = y_pred[mask]

        # (5) í‰ê°€ ì§€í‘œ  ------------------------------
        with np.errstate(divide="ignore", invalid="ignore"):
            r_true = np.diff(np.log(np.maximum(y_true, EPS)))
            r_pred = np.diff(np.log(np.maximum(y_pred, EPS)))

        mcc   = matthews_corrcoef(np.sign(r_true), np.sign(r_pred))
        smape = np.mean(np.abs(r_pred - r_true) /
                        (np.abs(r_pred) + np.abs(r_true) + EPS))

        reward    = _composite_reward(mcc, smape)
        new_alpha = _update_alpha(model.alpha, reward)
        model.set_params(alpha=new_alpha)

        # (6) ê²°ê³¼ ê¸°ë¡  ------------------------------
        results.append(
            BatchResult(
                train_period=(
                    train_b["Date"].min().strftime("%Y-%m-%d"),
                    train_b["Date"].max().strftime("%Y-%m-%d"),
                ),
                pred_period=(
                    pred_b["Date"].min().strftime("%Y-%m-%d"),
                    pred_b["Date"].max().strftime("%Y-%m-%d"),
                ),
                mcc=mcc,
                smape=smape,
                reward=reward,
                y_true=y_true,
                y_pred=y_pred,
            )
        )

        if verbose:
            print(
                tabulate(
                    [
                        ["MCC",   f"{mcc:+.4f}"],
                        ["SMAPE", f"{smape*100:.2f}%"],
                        ["Reward",f"{reward:.4f}"],
                        ["Î±",     f"{new_alpha:.8f}"],
                    ],
                    headers=["Metric", "Value"],
                    tablefmt="github",
                )
            )

    return model, scaler, results

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  DB ì €ì¥  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def save_results_bulk(results: List[BatchResult]):
    objs = []
    for r in results:
        if any(np.isnan([r.mcc, r.smape, r.reward])):
            continue
        objs.append(
            AI_REWARD2(
                MCC    = max(min(r.mcc,  1.0), -1.0),
                SMAPE  = min(r.smape, 1.0),
                REWARD = r.reward,
            )
        )
    if objs:
        AI_REWARD2.objects.bulk_create(objs)
        LOGGER.info(f"âœ… Saved {len(objs)} rows â†’ AI_REWARD")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  ë©”ì¸  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MODEL_PATH  = Path("models/price_model.pkl")
SCALER_PATH = Path("models/price_scaler.pkl")

def main():
    ticker = "QQQ"
    batches = list(walk_forward_batches(mini_batch_normalization(ticker)))
    DEBUG_FIRST_BATCH = False  # ğŸ”„ ë””ë²„ê·¸ìš© ì²« ë°°ì¹˜ ì¶œë ¥
    DEBUG_NOISE = True      # ğŸ”„ ë…¸ì´ì¦ˆ ì£¼ì… ë””ë²„ê·¸ (True ì‹œ ë…¸ì´ì¦ˆ ì œê±°)

    # ğŸ”„ ëª¨ë¸Â·ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ (ìµœì´ˆ 1íšŒ)
    model  = load_model(MODEL_PATH) if MODEL_PATH.exists() else None
    scaler = load_or_init_scaler(SCALER_PATH, FIT_COLS)

    # ğŸ”„ ëª¨ë¸ ì´ˆê¸°í™”
    if model is None:
        print("\nâš ï¸  ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        model = SGDRegressor(
            loss="huber",
            epsilon=1.35,
            random_state=42,
            penalty="l2",
            alpha=1e-4,
            learning_rate="invscaling",
            eta0=0.01,
            power_t=0.25,
            fit_intercept=True,
        )

    # ğŸ”„ ëª¨ë¸ ì´ˆê¸° ìƒíƒœ ë””ë²„ê·¸
    print("\nğŸ”„ ëª¨ë¸ ì´ˆê¸° ìƒíƒœ:")
    print(f"Alpha: {model.alpha}")
    print(f"Coef: {model.coef_[:5] if hasattr(model, 'coef_') else 'Not initialized'}")
    print(f"Intercept: {model.intercept_ if hasattr(model, 'intercept_') else 'Not initialized'}")

    # ğŸ”„ ë°°ì¹˜ í•™ìŠµ ì‹œì‘
    while True:
        model, scaler, results = partial_fit_batches(batches, model=model, scaler=scaler, verbose=True)

        # ğŸ”„ ê²°ê³¼ ë””ë²„ê·¸
        if results:
            sample_result = results[-1]
            print("\nğŸ“  [ë””ë²„ê·¸] ìµœê·¼ ë°°ì¹˜ ê²°ê³¼")
            print(f"Train Period: {sample_result.train_period}")
            print(f"Pred Period: {sample_result.pred_period}")
            print(f"MCC: {sample_result.mcc:.4f}")
            print(f"SMAPE: {sample_result.smape:.4f}")
            print(f"Reward: {sample_result.reward:.4f}")
            print(f"y_true[-5:]: {sample_result.y_true}")
            print(f"y_pred[-5:]: {sample_result.y_pred}")
    

        # ğŸ”„ ëª¨ë¸Â·ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ (ì£¼ê¸°ì  ì €ì¥ ê³ ë ¤)
        save_model(model, MODEL_PATH)
        save_scaler(scaler, SCALER_PATH)
        save_results_bulk(results)

        print("\nâœ… ëª¨ë¸Â·ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ ì™„ë£Œ\n")

        # ğŸ”„ ë¬´í•œ ë£¨í”„ ì¢…ë£Œ ì˜µì…˜ (í•„ìš” ì‹œ)
        if not DEBUG_NOISE:
            break


        

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("Interrupted â€“ shutting down cleanly")
