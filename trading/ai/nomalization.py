from __future__ import annotations
import pandas as pd
from pandas.tseries.offsets import CustomBusinessDay
import sys
import os
import django
from sklearn.linear_model import SGDRegressor
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.base import BaseEstimator, TransformerMixin
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.offline as pyo
import numpy as np

# Django í”„ë¡œì íŠ¸ ì„¤ì •
PROJECT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, PROJECT_DIR)
os.environ.setdefault("DJANGO_SETTINGS_MODULE", "backend.settings")
django.setup()
from history.models import Trade
import random
import matplotlib.pyplot as plt
import joblib
from datetime import timedelta
import logging
logger = logging.getLogger(__name__)
import random
KR_US_BDAY = CustomBusinessDay(calendar="KRX")  # í•„ìš” ì‹œ NYSE ì¼ì • ë³‘í•©
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score



## ì „ì—­ ë³€ìˆ˜ / í–¥í›„ ë³´ì¡°ì§€í‘œ ì¶”ê°€ì‹œ ë³€ê²½ ì˜ˆì •
fit_cols = ["Open","High","Low","Volume","Prev_Close"] + [f"SMA_{p}" for p in (5,20,60)]



# Django ëª¨ë¸ì—ì„œ ì£¼ê°€ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜
def trades_to_dataframe(ticker):
    trades = list(Trade.objects.filter(Ticker=ticker).values())
    if not trades:
        print(f"ë°ì´í„°ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {ticker}")
        return pd.DataFrame()
    df = pd.DataFrame(trades)

    # Date í˜•ì‹ ë³€í™˜
    if 'Date' in df.columns:
        df['Date'] = pd.to_datetime(df['Date'])
    
    # Float ë³€í™˜ (Decimal to float)
    for col in ["Open", "High", "Low", "Close", "Volume"]:
        if col in df.columns:
            df[col] = df[col].astype(float)

    #print(f"{ticker} ë°ì´í„° ë¡œë“œ ì™„ë£Œ (ì´ {len(df)} ê°œ)")
    return df


# ë°ì´í„°í”„ë ˆì„ì„ ê°œì›” ë‹¨ìœ„ë¡œ ë¯¸ë‹ˆ ë°°ì¹˜ë¡œ ë¶„í• í•˜ëŠ” ì œë„ˆë ˆì´í„° í•¨ìˆ˜
# ë””í´íŠ¸: 4ê°œì›”
def batch_generator(df, batch_month=4):
    if df.empty:
        print("ë°ì´í„°ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        return

    # ë‚ ì§œ í•„ë“œê°€ ì—†ìœ¼ë©´ ì¤‘ë‹¨
    if 'Date' not in df.columns:
        print("ë°ì´í„°í”„ë ˆì„ì— 'Date' í•„ë“œê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        return

    # ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
    df = df.sort_values(by='Date').reset_index(drop=True)

    # ì›” ë‹¨ìœ„ë¡œ ë¶„í• 
    start_date = df['Date'].min()
    end_date = df['Date'].max()
    current_date = start_date

    while current_date < end_date:
        # ë‹¤ìŒ ë°°ì¹˜ì˜ ë ë‚ ì§œ ê³„ì‚°
        next_date = current_date + pd.DateOffset(months=batch_month)
        
        # í˜„ì¬ ë°°ì¹˜ ì¶”ì¶œ
        batch = df[(df['Date'] >= current_date) & (df['Date'] < next_date)]
        
        if not batch.empty:
            yield batch
        
        # ë‹¤ìŒ ë°°ì¹˜ë¡œ ì´ë™
        current_date = next_date


# ì œë„ˆë ˆì´í„°ë¡œ ìƒì„±ëœ ë°°ì¹˜ë¥¼ ì…”í”Œí•˜ëŠ” í•¨ìˆ˜
def shuffle_batches(generator):

    # ëª¨ë“  ë°°ì¹˜ë¥¼ ì¼ë‹¨ ë¦¬ìŠ¤íŠ¸ì— ë¡œë“œ
    batches = list(generator)
    
    # ì…”í”Œ
    random.shuffle(batches)
    
    # ì…”í”Œëœ ë°°ì¹˜ë¥¼ ë‹¤ì‹œ ì œë„ˆë ˆì´í„°ë¡œ ë°˜í™˜
    for batch in batches:
        yield batch


# Mini-batch ë‹¨ìœ„ë¡œ MinMax Scalingë§Œ ì ìš© (ë°°ì¹˜ë³„ ë…ë¦½ ìŠ¤ì¼€ì¼ë§)
def normalize_batch(batch, columns=None, save_scaler_flag=True, scaler_filename="models/price_scaler.pkl"):

    if columns is None:
        columns = ["Open", "High", "Low", "Close", "Volume"]

    if batch.empty:
        return batch, None, None, None  # ë¹ˆ ë°°ì¹˜ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜

    # ìŠ¤ì¼€ì¼ëŸ¬ ìƒì„±
    scaler = MinMaxScaler()

    # ë°°ì¹˜ë³„ ìµœì†Œ, ìµœëŒ€ê°’ ì €ì¥ (ì—­ìŠ¤ì¼€ì¼ë§ì„ ìœ„í•´ í•„ìš”)
    min_vals = batch.loc[:, columns].min()
    max_vals = batch.loc[:, columns].max()

    # ë°°ì¹˜ë³„ ë…ë¦½ ìŠ¤ì¼€ì¼ë§
    scaled_values = scaler.fit_transform(batch.loc[:, columns])

    # ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„° ì ìš©
    scaled_batch = batch.copy()
    scaled_batch.loc[:, columns] = scaled_values

    # ** ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ **
    if save_scaler_flag:
        if isinstance(scaler, (BaseEstimator, TransformerMixin)):
            save_scaler(scaler, scaler_filename)
        else:
            raise TypeError(f"ì €ì¥í•˜ë ¤ëŠ” ê°ì²´ê°€ ìŠ¤ì¼€ì¼ëŸ¬ê°€ ì•„ë‹™ë‹ˆë‹¤: {type(scaler)}")

    # ì»¬ëŸ¼ ì´ë¦„ í¬í•¨í•˜ì—¬ ë°˜í™˜
    return scaled_batch, scaler, min_vals, max_vals


# í•™ìŠµëœ ìŠ¤ì¼€ì¼ëŸ¬ë¥¼ ë””ìŠ¤í¬ì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜
def save_scaler(scaler, filepath="models/price_scaler.pkl"):

    if not isinstance(scaler, (BaseEstimator, TransformerMixin)):
        raise TypeError("ìŠ¤ì¼€ì¼ëŸ¬ëŠ” scikit-learnì˜ TransformerMixinì„ ìƒì†í•´ì•¼ í•©ë‹ˆë‹¤.")
    
    # í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±
    os.makedirs(os.path.dirname(filepath), exist_ok=True)
    
    # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
    joblib.dump(scaler, filepath)
    print(f"ìŠ¤ì¹¼ë¼ ì €ì¥ ì™„ë£Œ")


# ì¥ëœ ìŠ¤ì¼€ì¼ëŸ¬ë¥¼ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
def load_scaler(filepath="models/price_scaler.pkl"):

    if not os.path.isfile(filepath):
        raise FileNotFoundError(f"ì§€ì •ëœ ìŠ¤ì¼€ì¼ëŸ¬ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {filepath}")
    
    # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
    scaler = joblib.load(filepath)

    # ìŠ¤ì¼€ì¼ëŸ¬ íƒ€ì… í™•ì¸
    if not isinstance(scaler, (BaseEstimator, TransformerMixin)):
        raise TypeError(f"ë¡œë“œëœ ê°ì²´ëŠ” ìŠ¤ì¼€ì¼ëŸ¬ê°€ ì•„ë‹™ë‹ˆë‹¤: {type(scaler)}")
    
    return scaler

# ëª¨ë¸ ì €ì¥ í•¨ìˆ˜
def save_model(model, filepath="models/price_model.pkl"):
    # BaseEstimator ìƒì† ì—¬ë¶€ ì²´í¬
    if not isinstance(model, BaseEstimator):
        print(f"ëª¨ë¸ íƒ€ì…ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤: {type(model)}")
        raise TypeError("ëª¨ë¸ì€ scikit-learnì˜ BaseEstimatorë¥¼ ìƒì†í•´ì•¼ í•©ë‹ˆë‹¤.")

    # ëª¨ë¸ ì €ì¥
    with open(filepath, "wb") as f:
        joblib.dump(model, filepath)
    print(f"ëª¨ë¸ ì €ì¥ ì™„ë£Œ")


# ì €ì¥ëœ ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
def load_model(filepath="models/price_model.pkl"):

    if not os.path.isfile(filepath):
        raise FileNotFoundError(f"ì§€ì •ëœ ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {filepath}")
    
    # ëª¨ë¸ ë¡œë“œ
    model = joblib.load(filepath)
    print(f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

    # ëª¨ë¸ íƒ€ì… í™•ì¸
    if not isinstance(model, BaseEstimator):
        raise TypeError(f"ë¡œë“œëœ ê°ì²´ëŠ” ëª¨ë¸ì´ ì•„ë‹™ë‹ˆë‹¤: {type(model)}")
    
    return model

def add_sma_features(df: pd.DataFrame, periods=(5, 20, 60)) -> pd.DataFrame:
    """
    ë‹¨ìˆœì´ë™í‰ê· (SMA) ì»¬ëŸ¼ì„ dfì— ì¶”ê°€í•œë‹¤.
    ì˜ˆ) periods=(5,20,60) â†’ 'SMA_5', 'SMA_20', 'SMA_60' ì»¬ëŸ¼ ìƒì„±
    """
    df = df.sort_values("Date").copy()
    for p in periods:
        df[f"SMA_{p}"] = df["Close"].rolling(window=p, min_periods=1).mean()
    return df



def mini_batch_scaling(
    batch_generator,
    batch_month=3,
    visualize=True,
    scaler_filename="models/price_scaler.pkl",
    sma_periods=(5, 20, 60)      # â† ì¶”ê°€: SMA ê¸°ê°„
):
    # 0) ìŠ¤ì¼€ì¼ë§ ëŒ€ìƒ ê¸°ë³¸ ì»¬ëŸ¼ + SMA ì»¬ëŸ¼ ---------------------
    base_cols = ["Open", "High", "Low", "Close", "Volume"]
    sma_cols  = [f"SMA_{p}" for p in sma_periods]
    columns   = base_cols + sma_cols         # ìŠ¤ì¼€ì¼ ëŒ€ìƒ ìµœì¢… ëª©ë¡

    scaled_batches, batch_ranges = [], []

    for i, raw_batch in enumerate(batch_generator):
        if raw_batch.empty:
            continue

        # 1) SMA ì»¬ëŸ¼ ì¶”ê°€ -------------------------------------
        batch = add_sma_features(raw_batch, periods=sma_periods)

        # 2) íƒ€ì… ë³€í™˜ í›„ MinMax ìŠ¤ì¼€ì¼ ------------------------
        batch = batch.copy()
        batch.loc[:, columns] = batch.loc[:, columns].astype(float)

        save_scaler_flag = (i == 0)
        scaled_batch, scaler, min_vals, max_vals = normalize_batch(
            batch,
            columns=columns,
            save_scaler_flag=save_scaler_flag,
            scaler_filename=scaler_filename,
        )
        scaled_batches.append(scaled_batch)
        batch_ranges.append((min_vals, max_vals))

        if visualize and i % 10 == 0:
            print(f"\n[Batch {i+1}] {batch['Date'].min()} ~ {batch['Date'].max()}")
            print(scaled_batch[columns].head())

    return scaled_batches, batch_ranges



# ì œë„ˆë ˆì´í„° í˜•ì‹ì˜ ë°°ì¹˜ì—ì„œ (train_b, pred_b) ìŒì„ ìƒì„±.
def walk_forward_batches(batches, train_months=3, pred_months=1):
    # ì œë„ˆë ˆì´í„°ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    batches = list(batches)
    # ì…”í”Œ
    random.shuffle(batches)

    # ê° ë°°ì¹˜ ë‚´ë¶€ ì •ë ¬
    for batch in batches:
        batch = batch.sort_values("Date").reset_index(drop=True)
        # ë°°ì¹˜ ìª¼ê°œê¸°
        cursor = batch["Date"].min()
        end    = batch["Date"].max()


        from pandas.tseries.offsets import CustomBusinessDay

        KR_US_BDAY = CustomBusinessDay(calendar="KRX")
        
        while cursor < end:
            # 4ê°œì›” ë°°ì¹˜
            batch_end = cursor + pd.DateOffset(months=4)

            full_batch = batch[(batch["Date"] >= cursor) & (batch["Date"] < batch_end)]
            
            # 3ê°œì›” í•™ìŠµ + 1ê°œì›” ì˜ˆì¸¡
            train_end = cursor + pd.DateOffset(months=3)
            train_batch = full_batch[(full_batch["Date"] >= cursor) & (full_batch["Date"] < train_end)]

            pred_batch = full_batch[(full_batch["Date"] >= train_end) & (full_batch["Date"] < batch_end)]

            # ì˜ˆì¸¡ ë°ì´í„°ê°€ ë¶€ì¡±í•œ ê²½ìš° ìƒì„±
            if pred_batch.empty:
                pred_dates = pd.date_range(
                    start=train_end, 
                    end=batch_end - pd.DateOffset(days=1), 
                    freq=KR_US_BDAY
                )
                pred_batch = pd.DataFrame({
                    "Date": pred_dates,
                    "id": [None] * len(pred_dates),
                    "Ticker": [train_batch["Ticker"].iloc[0]] * len(pred_dates),
                    "Open": [None] * len(pred_dates),
                    "High": [None] * len(pred_dates),
                    "Low": [None] * len(pred_dates),
                    "Close": [None] * len(pred_dates),
                    "Volume": [None] * len(pred_dates),
                })

            # ë¹ˆ ë°°ì¹˜ëŠ” ê±´ë„ˆë›°ê¸°
            if train_batch.empty:
                break

            yield train_batch, pred_batch if not pred_batch.empty else None
            cursor = batch_end  # 4ê°œì›” ë‹¨ìœ„ ì´ë™


# ê·¸ë˜í”„ ê·¸ë¦¬ëŠ” í•¨ìˆ˜ => í–¥í›„ Djangoë¡œ ì „í™˜ ì˜ˆì •
def plot_predictions(y_pred, y_actual, dates=None, batch_index=1, filename="predictions.html"):
    """
    ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì„ ì‹œê°í™”í•˜ëŠ” í•¨ìˆ˜ (Plotly ë²„ì „, ì›¹ ì¶œë ¥)
    """
    # ê¸°ë³¸ xì¶•: Sample Index
    x = np.arange(len(y_pred)) if dates is None else dates
    
    # ì„œë¸Œí”Œë¡¯ ìƒì„±
    fig = make_subplots(rows=1, cols=1, subplot_titles=[f"Batch {batch_index} - Predictions vs Actuals"])

    # ì˜ˆì¸¡ê°’ í”Œë¡¯
    fig.add_trace(
        go.Scatter(x=x, y=y_pred, mode='lines+markers', name="Predicted", opacity=0.7),
        row=1, col=1
    )

    # ì‹¤ì œê°’ í”Œë¡¯ (ìˆì„ ë•Œë§Œ)
    if y_actual:
        fig.add_trace(
            go.Scatter(x=x, y=y_actual, mode='lines+markers', name="Actual", opacity=0.7),
            row=1, col=1
        )

    # ë ˆì´ì•„ì›ƒ ì„¤ì •
    fig.update_layout(
        title_text=f"Batch {batch_index} - Predictions vs Actuals",
        xaxis_title="Date" if dates is not None else "Sample Index",
        yaxis_title="Price (USD)",
        legend_title="Legend",
        height=600,
        width=1200,
        xaxis_tickformat="%Y-%m-%d" if dates is not None else None  # ë‚ ì§œ í¬ë§·
    )

    # ê·¸ë˜í”„ ë Œë”ë§
    fig.show()

    print(f"âœ… ì°¨íŠ¸ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {os.path.abspath(filename)}")


# ë°°ì¹˜ë³„ë¡œ ë…ë¦½ì ìœ¼ë¡œ ìŠ¤ì¼€ì¼ë§ëœ ë°°ì¹˜ë¥¼ ì—­ì •ê·œí™” í›„ ì—­ìŠ¤ì¼€ì¼ë§í•˜ëŠ” í•¨ìˆ˜
def inverse_scale_batch(scaled_batch, min_vals, max_vals, columns=None, normalization_method=None, lambdas=None):

    if columns is None:
        columns = ["Open", "High", "Low", "Close", "Volume"]
    
    # ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„°ë§Œ ì¶”ì¶œ
    scaled_values = scaled_batch.loc[:, columns].values
    
    # ì—­ì •ê·œí™” (í•„ìš”í•œ ê²½ìš°)
    if normalization_method == "log":
        scaled_values = np.exp(scaled_values)
    elif normalization_method == "log1p":
        scaled_values = np.expm1(scaled_values)
    elif normalization_method == "boxcox":
        if lambdas is None or not isinstance(lambdas, dict):
            raise ValueError("Box-Cox ë³µì›ì„ ìœ„í•´ ê° í”¼ì²˜ì˜ ëŒë‹¤ ê°’ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        restored_values = np.zeros_like(scaled_values)
        for i, col in enumerate(columns):
            lambda_val = lambdas.get(col)
            if lambda_val is None:
                raise ValueError(f"Box-Cox ë³µì›ì„ ìœ„í•´ '{col}'ì˜ ëŒë‹¤ ê°’ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            if lambda_val == 0:
                restored_values[:, i] = np.exp(scaled_values[:, i])  # Î»=0 ì¼ ë•ŒëŠ” ë¡œê·¸ ë³€í™˜ê³¼ ë™ì¼
            else:
                restored_values[:, i] = np.power(scaled_values[:, i] * lambda_val + 1, 1 / lambda_val)
        scaled_values = restored_values
    
    elif normalization_method == "yeo-johnson":
        if lambdas is None or not isinstance(lambdas, dict):
            raise ValueError("Yeo-Johnson ë³µì›ì„ ìœ„í•´ ê° í”¼ì²˜ì˜ ëŒë‹¤ ê°’ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        restored_values = np.zeros_like(scaled_values)
        for i, col in enumerate(columns):
            lambda_val = lambdas.get(col)
            if lambda_val is None:
                raise ValueError(f"Yeo-Johnson ë³µì›ì„ ìœ„í•´ '{col}'ì˜ ëŒë‹¤ ê°’ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            # Yeo-Johnson ë³µì›
            if lambda_val == 0:
                restored_values[:, i] = np.exp(scaled_values[:, i]) - 1
            else:
                pos_idx = scaled_values[:, i] >= 0
                neg_idx = ~pos_idx
                restored_values[pos_idx, i] = np.power(scaled_values[pos_idx, i] * lambda_val + 1, 1 / lambda_val) - 1
                restored_values[neg_idx, i] = 1 - np.power(-(scaled_values[neg_idx, i]) * lambda_val + 1, 1 / lambda_val)
        scaled_values = restored_values
    
    # ë°°ì¹˜ë³„ ìµœì†Œ-ìµœëŒ€ê°’ì„ ì‚¬ìš©í•œ ì—­ìŠ¤ì¼€ì¼ë§
    original_values = scaled_values * (max_vals.values - min_vals.values) + min_vals.values
    
    # ë³µì›ëœ ë°ì´í„°í”„ë ˆì„ ìƒì„± (ë‚ ì§œ ìœ ì§€)
    restored_batch = scaled_batch.copy()
    restored_batch.loc[:, columns] = original_values
    
    return restored_batch


# ë³µì›ëœ ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì„ ë¹„êµí•˜ëŠ” í•¨ìˆ˜
def compare_predictions_with_actuals(restored_batch, original_df, columns=None):

    if columns is None:
        columns = ["Open", "High", "Low", "Close", "Volume"]
    
    # ë‚ ì§œì™€ í‹°ì»¤ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©
    merged_df = pd.merge(
        restored_batch,
        original_df[["Date", "Ticker"] + columns],
        on=["Date", "Ticker"],
        suffixes=("_pred", "_actual")
    )

    # ë¹„êµ ì¶œë ¥
    print("\nì˜ˆì¸¡ê°’ vs ì‹¤ì œê°’ (ì¼ë¶€):")
    print(merged_df.head())
    
    return merged_df


# ì‹¤ì œ ì˜ˆì¸¡
def predict_real_data(
    ticker: str,
    model: BaseEstimator,
    *,
    days_ahead: int = 21,
    scaler: MinMaxScaler | None = None,
    scaler_filename: str = "models/price_scaler.pkl",
    lookback_months: int = 3,
    calendar: CustomBusinessDay = KR_US_BDAY,
) -> pd.DataFrame:
    """ìµœê·¼ lookback_months ë°ì´í„°ë¡œ ìŠ¤ì¼€ì¼ì„ ë§ì¶˜ ë’¤ days_ahead ì˜ì—…ì¼ì„ ë‹¤ë‹¨ ì˜ˆì¸¡."""

    # 0) ìŠ¤ì¼€ì¼ëŸ¬ í™•ë³´ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    scaler = scaler or load_or_init_scaler(scaler_filename, fit_cols)

    # 1) ìµœê·¼ nê°œì›” ë°ì´í„° ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    df = trades_to_dataframe(ticker)
    if df.empty:
        logger.warning("(%s) ë°ì´í„° ì—†ìŒ", ticker)
        return pd.DataFrame(columns=["Date", "Pred_Close"])

    cutoff = pd.Timestamp.utcnow() - pd.DateOffset(months=lookback_months)
    recent = df[df["Date"] >= cutoff].copy()
    if recent.empty:
        logger.warning("âŒ (%s) ìµœê·¼ %dê°œì›” ë°ì´í„° ì—†ìŒ", ticker, lookback_months)
        return pd.DataFrame(columns=["Date", "Pred_Close"])

    # 2) Prev_Close ìƒì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    recent["Prev_Close"] = recent["Close"].shift(1).ffill()

    # 3) ìŠ¤ì¼€ì¼ & í”¼ì²˜ ì •ë ¬ í™•ì¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    feature_cols = model.feature_names_in_.tolist()
    if feature_cols != fit_cols:
        raise ValueError(f"ëª¨ë¸ í”¼ì²˜ {feature_cols} â†” ì˜ˆìƒ {fit_cols} ë¶ˆì¼ì¹˜")

    # 4) ìŠ¤ì¼€ì¼ëŸ¬ fit ì—¬ë¶€ í™•ì¸ â†’ ë¯¸-fit ì‹œ ìµœê·¼ ë°ì´í„°ë¡œ fit â”€â”€
    if not hasattr(scaler, "data_min_"):
        scaler.fit(recent[feature_cols])
        joblib.dump(scaler, scaler_filename)
        print("ğŸ†•  ìŠ¤ì¼€ì¼ëŸ¬ë¥¼ ìµœê·¼ ë°ì´í„°ë¡œ fit & ì €ì¥")

    # 5) ì‹œì‘í–‰ ì¤€ë¹„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    X_scaled = scaler.transform(recent[feature_cols])
    last_row = X_scaled[-1].reshape(1, -1)          # (1, n_features)
    last_date = recent["Date"].max()
    idx_prev  = feature_cols.index("Prev_Close")
    n_noise   = idx_prev                           # Open~Volume ì»¬ëŸ¼ ê°œìˆ˜

    preds, dates = [], []

    # 6) ë‹¤ì¤‘ ìŠ¤í… ì˜ˆì¸¡ ë£¨í”„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    for _ in range(days_ahead):
        # 6-1) ì˜ˆì¸¡ (DataFrameìœ¼ë¡œ ì „ë‹¬í•´ ê²½ê³  ì œê±°)
        last_row_df = pd.DataFrame(last_row, columns=feature_cols)
        next_scaled_close = float(model.predict(last_row_df)[0])

        # 6-2) full_scaled_row ì‘ì„± (Prev_Close ìë¦¬ë§Œ êµì²´)
        full_scaled_row = last_row.copy()
        full_scaled_row[0, idx_prev] = next_scaled_close

        # 6-3) ì—­-ìŠ¤ì¼€ì¼ â†’ ì‹¤ì œ Close ê°’
        next_close = float(
            scaler.inverse_transform(full_scaled_row)[0, idx_prev]
        )
        preds.append(next_close)

        # 6-4) ë‹¤ìŒ ì˜ì—…ì¼ ë‚ ì§œ
        last_date += calendar
        dates.append(last_date)

        # 6-5) ì…ë ¥ í–‰ ì—…ë°ì´íŠ¸
        last_row = full_scaled_row  # â† Prev_Close í¬í•¨ ì „ì²´ í”¼ì²˜ êµì²´
        # íƒìƒ‰ ë…¸ì´ì¦ˆ: OpenÂ·HighÂ·LowÂ·Volume ì— ì‘ì€ ë³€ë™ ì¶”ê°€
        noise = np.random.normal(0.0, 0.01, size=n_noise)
        last_row[0, :n_noise] += noise

    # 7) ê²°ê³¼ & ì‹œê°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    result = pd.DataFrame({"Date": dates, "Pred_Close": preds})
    logger.info("(%s) +%d ì˜ì—…ì¼ ì˜ˆì¸¡ ì™„ë£Œ", ticker, days_ahead)

    plot_predictions(
        y_pred=result["Pred_Close"].values,
        y_actual=[],            # ì‹¤ì œê°’ ì—†ìŒ
        dates=result["Date"].tolist(),
        batch_index=1,
    )
    return result


def load_or_init_scaler(path: str, fit_cols: list[str]) -> MinMaxScaler:
    """ìŠ¤ì¼€ì¼ëŸ¬ë¥¼ ë¡œë“œí•˜ê±°ë‚˜, ì—†ìœ¼ë©´ â€˜ë‚˜ì¤‘ì—â€™ fit í•  ë¹ˆ ìŠ¤ì¼€ì¼ëŸ¬ë¥¼ ë°˜í™˜."""
    try:
        sc = load_scaler(path)
        if (not hasattr(sc, "feature_names_in_") or
            list(sc.feature_names_in_) != fit_cols):
            print("âš ï¸  ìŠ¤ì¼€ì¼ëŸ¬-í”¼ì²˜ ë¶ˆì¼ì¹˜ â†’ ìƒˆ ìŠ¤ì¼€ì¼ëŸ¬ ìƒì„±")
            sc = MinMaxScaler()
    except FileNotFoundError:
        print("ğŸ†•  ìŠ¤ì¼€ì¼ëŸ¬ íŒŒì¼ ì—†ìŒ â†’ ìƒˆ ìŠ¤ì¼€ì¼ëŸ¬ ìƒì„±")
        sc = MinMaxScaler()

    sc.feature_names_in_ = np.array(fit_cols)  # ì´ë¦„ë§Œ ë¯¸ë¦¬ ì„¸íŒ…
    return sc



# ì…”í”Œê¹Œì§€ ë§ˆì¹œ ë°°ì¹˜ë¥¼ ë‚´ë³´ë‚´ëŠ” ëª¨ë“ˆ
def mini_batch_normalization(ticker):
    # ticker = "QQQ"

    # 1ë‹¨ê³„: ì „ì²´ ë°ì´í„° í”„ë ˆì„ ë§Œë“¤ê¸°
    df = trades_to_dataframe(ticker)

    # 2ë‹¨ê³„: ë°°ì¹˜ ì œë„ˆë ˆì´í„°
    batch_gen = batch_generator(df, batch_month=4)

    # 3ë‹¨ê³„: ë°°ì¹˜ ìŠ¤ì¼€ì¼ë§ + ì •ê·œí™” (ìŠ¤ì¼€ì¼ë§ì´ ë¨¼ì €)
    scaled_batches, batch_ranges = mini_batch_scaling(
        batch_gen,
        visualize=False               # save_scaler_flag ì¸ì âŒ ì œê±°
    )
    # 4ë‹¨ê³„: ë°°ì¹˜ ìˆœì„œ ì„ê¸°
    shuffled_batches = list(shuffle_batches(iter(scaled_batches)))

    return shuffled_batches
    