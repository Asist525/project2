"""
Bi-LSTM + Attention + Monte-Carlo (CPU-only, dynamic LR schedule)
íŒŒì¼ëª…: lstm_mc_cpu_dynamic.py
"""
from __future__ import annotations
import os, json, logging, sys
import numpy as np
import pandas as pd

# â”€â”€â”€ í˜¸í™˜ì„± íŒ¨ì¹˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if not hasattr(np, "NaN"):
    np.NaN = np.nan
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import TimeSeriesSplit
from tensorflow.keras.losses import Huber
import tensorflow as tf
from tensorflow.keras.layers import (
    Conv1D, LSTM, Dense, Dropout, BatchNormalization, 
    LayerNormalization, Input, Bidirectional, MultiHeadAttention, Add, Concatenate
)
import matplotlib.pyplot as plt
import matplotlib
import pandas_ta as ta   # ê¸°ìˆ ì  ì§€í‘œ
from nomalization import *
matplotlib.use("Agg")    # GUI ì—†ëŠ” í™˜ê²½ ëŒ€ë¹„
os.environ["CUDA_VISIBLE_DEVICES"] = "-1"
os.environ["TF_XLA_FLAGS"] = "--tf_xla_enable_xla_devices=false"
tf.config.optimizer.set_jit(False)




# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1. ë°ì´í„° ì „ì²˜ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def add_ta_features(df: pd.DataFrame) -> pd.DataFrame:
    """ë‹¨ê¸° ë°©í–¥ì„± ê°œì„ ì„ ìœ„í•œ TA í”¼ì²˜ ì¶”ê°€"""
    df_ta = df.copy()

    # â”€â”€ ê¸°ì¡´ ì§€í‘œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    df_ta.ta.rsi(length=14, append=True)
    df_ta.ta.sma(length=5, append=True)
    df_ta.ta.sma(length=20, append=True)

    # â”€â”€ ì´ˆë‹¨ê¸° ê°€ê²© ë³€í™” (ROCÂ·Momentum) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    df_ta["1d_ROC"] = df_ta["Close"].pct_change(1)
    df_ta["3d_ROC"] = df_ta["Close"].pct_change(3)
    df_ta["5d_ROC"] = df_ta["Close"].pct_change(5)

    df_ta["1d_Momentum"] = df_ta["Close"].diff(1)
    df_ta["3d_Momentum"] = df_ta["Close"].diff(3)
    df_ta["5d_Momentum"] = df_ta["Close"].diff(5)

    # â”€â”€ Williams %R(5) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    df_ta["Williams_R_5"] = ta.willr(
        df_ta["High"], df_ta["Low"], df_ta["Close"], length=5
    )

    # â”€â”€ True Strength Index(TSI 5Â·20) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    tsi = ta.tsi(df_ta["Close"], fast=5, slow=20)      # pandas-ta ê°ì²´
    # pandas-ta ë²„ì „ì— ë”°ë¼ Series ë˜ëŠ” DataFrame ë°˜í™˜
    if isinstance(tsi, pd.DataFrame):
        df_ta["TSI"] = tsi.iloc[:, 0]                  # ì²« ì»¬ëŸ¼ë§Œ ì‚¬ìš©
    else:
        df_ta["TSI"] = tsi

    # â”€â”€ Money Flow Index(5) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    df_ta["MFI_5"] = ta.mfi(
        df_ta["High"], df_ta["Low"], df_ta["Close"], df_ta["Volume"], length=5
    )

    # â”€â”€ Volume Delta â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    df_ta["1d_Vol_Delta"] = df_ta["Volume"].diff(1)
    df_ta["3d_Vol_Delta"] = df_ta["Volume"].diff(3)

    # â”€â”€ NaNÂ·âˆ ì •ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    df_ta.replace([np.inf, -np.inf], np.nan, inplace=True)
    df_ta.dropna(inplace=True)

    return df_ta





def load_and_preprocess(
        ticker: str,
        lookback: int = 365,
        end_date: str | pd.Timestamp | None = None
) -> pd.DataFrame:
    """
    â€¢ lookback: ë’¤ì—ì„œë¶€í„° ê°€ì ¸ì˜¬ ì˜ì—…ì¼ ìˆ˜
    â€¢ end_date: í•´ë‹¹ ë‚ ì§œ(í¬í•¨) ê¸°ì¤€ìœ¼ë¡œ lookback ë§Œí¼ ì˜ë¼ì„œ ë°˜í™˜
                None ì´ë©´ ê°€ì¥ ìµœì‹  ì¼ìë¥¼ end_date ë¡œ ê°„ì£¼
    """
    from nomalization import trades_to_dataframe  # í”„ë¡œì íŠ¸ util

    # â”€â”€ â‘  ì›ë³¸ ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    df = trades_to_dataframe(ticker)
    df["Date"] = pd.to_datetime(df["Date"])
    df.set_index("Date", inplace=True)

    # â”€â”€ â‘¡ ë²”ìœ„ ìŠ¬ë¼ì´ì‹± (end_date + lookback) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if end_date is None:
        end_date = df.index.max()
    end_date = pd.Timestamp(end_date)

    start_idx = end_date - pd.tseries.offsets.BDay(lookback - 1)
    if end_date.tzinfo is None:
        end_date = end_date.tz_localize("UTC")

    start_idx = end_date - pd.tseries.offsets.BDay(lookback - 1)

    # start_idx ì—­ì‹œ tz-aware ë¡œ ë§ì¶”ê¸°
    start_idx = start_idx.tz_convert("UTC") if start_idx.tzinfo else start_idx.tz_localize("UTC")

    df = df.loc[start_idx:end_date]        # â† ì´ì œ tz ì¼ì¹˜

    # â”€â”€ â‘¢ ì „ì²˜ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    df.drop(columns=["Ticker"], inplace=True, errors="ignore")
    df["LogRet"] = np.log(df["Close"].pct_change() + 1)

    df["Prev_Close"] = df["Close"].shift(1)
    df = df.assign(
        SMA_5=df["Close"].rolling(5).mean(),
        SMA_20=df["Close"].rolling(20).mean(),
    )

    df = add_ta_features(df)
    df.replace([np.inf, -np.inf], np.nan, inplace=True)
    df.dropna(inplace=True)

    return df

def scale_features(df: pd.DataFrame, cols: list[str]):
    scaler = MinMaxScaler()
    scaled = scaler.fit_transform(df[cols])
    return scaled.astype(np.float32), scaler


def dynamic_weighted_huber(y_true, y_pred, alpha=0.95, delta=0.5, kappa=1.3):
    """
    â€¢ ì‹œê³„ì—´ ê° ì‹œì ì— ëŒ€í•´ ì ì§„ì  ê°€ì¤‘ì¹˜ ì ìš©
    â€¢ ë°©í–¥ì„± ì˜¤íŒ í˜ë„í‹° ì™„í™”
    """
    # ê°€ì¤‘ì¹˜ ë²¡í„° (ë‹¨ê¸° ~ ì¥ê¸°)
    steps = tf.shape(y_true)[1]
    weights = tf.linspace(1.0, alpha, steps)
    weights = tf.reshape(weights, (1, -1))  # (1, steps)

    # Huber ì†ì‹¤ ê³„ì‚°
    error = y_true - y_pred
    abs_err = tf.abs(error)
    huber_loss = tf.where(abs_err < delta, 0.5 * tf.square(error), delta * (abs_err - 0.5 * delta))

    # ë°©í–¥ì„± í˜ë„í‹° (ë¶€í˜¸ê°€ ë‹¤ë¥¸ ê²½ìš° kappa ë°° ê°€ì¤‘ì¹˜)
    direction_penalty = tf.cast(tf.sign(y_true) != tf.sign(y_pred), tf.float32)
    amplified_loss = huber_loss * (1 + direction_penalty * (kappa - 1))

    # ì‹œì ë³„ ê°€ì¤‘ì¹˜ ì ìš©
    weighted_loss = weights * amplified_loss

    # ìµœì¢… í‰ê·  ì†ì‹¤ ë°˜í™˜
    return tf.reduce_mean(weighted_loss)



def create_sequences(data: np.ndarray, target: np.ndarray, ts: int, pred_days: int):
    """
    ì‹œê³„ì—´ ë°ì´í„°ì—ì„œ (X, y) ì‹œí€€ìŠ¤ë¥¼ ìƒì„±
    â€¢ ts: ì…ë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´
    â€¢ pred_days: ì¶œë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´
    """
    X, y = [], []
    for i in range(len(data) - ts - pred_days + 1):
        X.append(data[i:i + ts])
        y.append(target[i + ts:i + ts + pred_days])

    # np.stack â†’ Ragged array ë°©ì§€
    return (np.stack(X).astype(np.float32),
            np.stack(y).astype(np.float32))


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 2. ëª¨ë¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def build_model(ts: int, n_feat: int, pred_days: int, total_steps: int):
    """
    ëª¨ë¸ êµ¬ì¡° (ì§§ì€, ì¤‘ê°„, ê¸´ Lookback í†µí•©)
    """
    inp = Input(shape=(ts, n_feat), name="Input_Layer")

    # â”€â”€ â‘  ì§§ì€ Lookback (20) - 2ì¸µ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    short = inp[:, -20:, :]
    x_short = Conv1D(16, 3, activation="relu", padding="causal", name="Short_Conv")(short)
    x_short = BatchNormalization(name="Short_BN")(x_short)
    x_short = Bidirectional(LSTM(32, return_sequences=True, dropout=0.2), name="Short_BiLSTM_1")(x_short)
    x_short = Bidirectional(LSTM(32, return_sequences=True, dropout=0.2), name="Short_BiLSTM_2")(x_short)
    x_short = LSTM(32, return_sequences=False, dropout=0.2, name="Short_LSTM")(x_short)

    # â”€â”€ â‘¡ ì¤‘ê°„ Lookback (60) - 2ì¸µ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    mid = inp[:, -60:, :]
    x_mid = Conv1D(32, 5, activation="relu", padding="causal", name="Mid_Conv")(mid)
    x_mid = BatchNormalization(name="Mid_BN")(x_mid)
    x_mid = Bidirectional(LSTM(64, return_sequences=True, dropout=0.2), name="Mid_BiLSTM_1")(x_mid)
    x_mid = Bidirectional(LSTM(64, return_sequences=True, dropout=0.2), name="Mid_BiLSTM_2")(x_mid)
    x_mid = LSTM(64, return_sequences=False, dropout=0.2, name="Mid_LSTM")(x_mid)

    # â”€â”€ â‘¢ ê¸´ Lookback (120) - 2ì¸µ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    long = inp[:, -120:, :]
    x_long = Conv1D(64, 7, activation="relu", padding="causal", name="Long_Conv")(long)
    x_long = BatchNormalization(name="Long_BN")(x_long)
    x_long = Bidirectional(LSTM(128, return_sequences=True, dropout=0.2), name="Long_BiLSTM_1")(x_long)
    x_long = Bidirectional(LSTM(128, return_sequences=True, dropout=0.2), name="Long_BiLSTM_2")(x_long)
    x_long = LSTM(128, return_sequences=False, dropout=0.2, name="Long_LSTM")(x_long)

    # â”€â”€ â‘£ Feature Merge (2ì¸µ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    merged = Concatenate(name="Merged_Features")([x_short, x_mid, x_long])
    x = Dense(256, activation="relu", name="Dense_1")(merged)
    x = Dropout(0.3, name="Dropout_1")(x)
    x = BatchNormalization(name="BN_1")(x)
    x = Dense(128, activation="relu", name="Dense_2")(x)
    x = Dropout(0.3, name="Dropout_2")(x)
    out = Dense(pred_days, name="Output_Layer")(x)

    # â”€â”€ â‘¤ Optimizer & Loss â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    decay_steps = int(total_steps * 0.8)
    lr_schedule = tf.keras.optimizers.schedules.CosineDecay(
        initial_learning_rate=1e-3,
        decay_steps=decay_steps,
        alpha=1e-2
    )
    opt = tf.keras.optimizers.AdamW(
        learning_rate=lr_schedule,
        weight_decay=1e-4,
        clipnorm=1.0
    )
    
    model = tf.keras.Model(inp, out, name="Multi_Path_LSTM_Model")
    model.compile(
        optimizer=opt,
        loss=lambda y, Å·: sign_aware_huber(y, Å·, delta=0.5, kappa=1.4),  # kappa ìˆ˜ì •
        metrics=["mae", "mape"]
    )
    
    return model




def sign_aware_huber(y_true, y_pred, delta=0.5, kappa=1.3):
    """
    Huber ì†ì‹¤ + ë°©í–¥ì„± íŒ¨ë„í‹° (ì™„í™”)
    """
    # ê¸°ë³¸ Huber ì†ì‹¤
    error = y_true - y_pred
    abs_error = tf.abs(error)
    huber_loss = tf.where(abs_error < delta, 
                          0.5 * tf.square(error), 
                          delta * (abs_error - 0.5 * delta))
    
    # ë°©í–¥ì„± íŒ¨ë„í‹° (ë¶€í˜¸ê°€ ë‹¤ë¥´ë©´ kappa ë°°)
    direction_mismatch = tf.cast(tf.sign(y_true) != tf.sign(y_pred), tf.float32)
    adjusted_loss = huber_loss * (1 + direction_mismatch * (kappa - 1))

    # ë°°ì¹˜ ì°¨ì›ë§Œ í‰ê·  (pred_days ì¶•)
    return tf.reduce_mean(adjusted_loss, axis=-1)





# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3. ë©”ì¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main(): 
    ticker = "QQQ"
    lookback = 412
    ts_len = 120
    horizon = 30
    end_date = "2024-07-18"
    
    # ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬
    df = load_and_preprocess(ticker, lookback, end_date)
    feat_cols = [c for c in df.columns if c not in ("Close", "LogRet", "Prev_Close")]
    Xs, scX = scale_features(df, feat_cols)
    ys, scY = scale_features(df, ["LogRet"])
    X, y = create_sequences(Xs, ys.flatten(), ts_len, horizon)

    # êµì°¨ ê²€ì¦
    tscv = TimeSeriesSplit(5)
    folds = list(tscv.split(X))
    
    # ë§ˆì§€ë§‰ FoldëŠ” ê²€ì¦ìš©ìœ¼ë¡œë§Œ ì‚¬ìš©
    train_folds = folds[:-1]   # ì• 4ê°œ (í›ˆë ¨)
    val_fold = folds[-1]       # ìµœì‹  1ê°œ (ê²€ì¦)

    best_val_loss = np.inf
    best_model = None
    
    # â”€â”€ â‘  í›ˆë ¨ (ìµœì‹  fold ì œì™¸) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    for fold_num, (tr_idx, _) in enumerate(train_folds, 1):
        print(f"\nğŸŒ€ Fold {fold_num}/{len(train_folds)} - Training Start")
        model = build_model(ts_len, len(feat_cols), horizon, int(np.ceil(len(tr_idx) / 32)) * 150)

        # 10ì—í­ ë‹¨ìœ„ ì†ì‹¤ ë¡œê·¸ ì¶œë ¥
        best_train_loss = np.inf
        for epoch in range(1, 151):
            history = model.fit(
                X[tr_idx], y[tr_idx],
                epochs=1, batch_size=32, verbose=0
            )

            # ì†ì‹¤ ì¶œë ¥
            tr_loss = history.history["loss"][-1]
            if epoch % 10 == 0:
                print(f"Fold {fold_num} | Epoch {epoch} - Training Loss: {tr_loss:.6f}")

            # ëª¨ë¸ ì—…ë°ì´íŠ¸ (ê²€ì¦ foldì—ì„œë§Œ)
            if tr_loss < best_train_loss:
                best_train_loss = tr_loss
                print(f"ğŸŸ¢ Training Model Updated (Best Training Loss: {best_train_loss:.6f})")
    
    # â”€â”€ â‘¡ ìµœì¢… ê²€ì¦ (ìµœì‹  fold) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\nğŸ§ª Final Fold - Validation Start")
    tr, va = val_fold
    model = build_model(ts_len, len(feat_cols), horizon, int(np.ceil(len(tr) / 32)) * 150)

    for epoch in range(1, 151):
        history = model.fit(
            X[tr], y[tr], validation_data=(X[va], y[va]),
            epochs=1, batch_size=32, verbose=0,
            callbacks=[tf.keras.callbacks.EarlyStopping(monitor="val_loss", patience=5, restore_best_weights=True)]
        )
        
        # ì†ì‹¤ ì¶œë ¥
        val_loss = history.history["val_loss"][-1]
        if epoch % 10 == 0:
            print(f"Final Fold | Epoch {epoch} - Validation Loss: {val_loss:.6f}")

        # ëª¨ë¸ ì—…ë°ì´íŠ¸ (ê²€ì¦ ë‹¨ê³„ì—ì„œë§Œ)
        if val_loss < best_val_loss:
            best_val_loss, best_model = val_loss, model
            print(f"ğŸ”µ Model Updated (Best Validation Loss: {best_val_loss:.6f})")

    # ìµœì  ëª¨ë¸ë¡œ ì˜ˆì¸¡
    last_window = Xs[-ts_len:].reshape(1, ts_len, -1)
    next_logrets = best_model.predict(last_window)[0]
    next_logrets_pred = scY.inverse_transform(next_logrets.reshape(-1, 1)).flatten()
    last_price = df["Close"].iloc[-1]
    next_prices = last_price * np.exp(next_logrets_pred.cumsum())

    # ì‹œê°í™”
    fc_dates = pd.date_range(end=df.index[-1], periods=horizon + 1, freq="B")[1:]
    plt.figure(figsize=(14, 8))
    plt.plot(fc_dates, next_prices, label="Forecast", lw=2)
    plt.title(f"{ticker} â€“ 30-Day Forecast")
    plt.ylabel("Price")
    plt.legend()
    plt.tight_layout()
    plt.savefig("forecast.png", dpi=300)
    plt.close()

    print("Next Prices:", next_prices)

if __name__ == "__main__":
    main()


